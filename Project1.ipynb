{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "import operator\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "\n",
    "#Load data in json format and store in a dataframe\n",
    "with open(\"proj1_data.json\") as fp:\n",
    "    data = json.load(fp)\n",
    "    df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert true-false to 1-0\n",
    "df[\"is_root\"] = df[\"is_root\"].astype(int)\n",
    "#Convert all text to lower cases\n",
    "df[\"text\"]= [x.lower() for x in df[\"text\"]]\n",
    "#Parse text where there's a space\n",
    "df[\"text\"]= [x.split() for x in df[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>children</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>is_root</th>\n",
       "      <th>popularity_score</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.254698</td>\n",
       "      <td>[its, raining, sideways]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.509813</td>\n",
       "      <td>[wheel, of, time, reader, confirmed!]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.370827</td>\n",
       "      <td>[the, jungle, book, of, pussy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.272843</td>\n",
       "      <td>[i'm, just, making, this, thread, since, there...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   children  controversiality  is_root  popularity_score  \\\n",
       "0         0                 0        0          1.254698   \n",
       "1         0                 0        0          0.509813   \n",
       "2         0                 0        1          0.370827   \n",
       "3         0                 0        0         -0.272843   \n",
       "\n",
       "                                                text  \n",
       "0                           [its, raining, sideways]  \n",
       "1              [wheel, of, time, reader, confirmed!]  \n",
       "2                     [the, jungle, book, of, pussy]  \n",
       "3  [i'm, just, making, this, thread, since, there...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First 10000 data points as training set\n",
    "train = df.iloc[0:10000,:]\n",
    "#10000 to 11000 as validation set\n",
    "validation = df.iloc[10000:11000,:]\n",
    "validation.index -= 10000\n",
    "#Last 1000 as test set\n",
    "test = df.iloc[11000:12000,:]\n",
    "test.index -= 11000\n",
    "#Display the first 10 points of the training data\n",
    "train.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset, nb_words):\n",
    "    \n",
    "    # Feature: Does the comment contain a question mark\n",
    "    qmarks = np.zeros((dataset.shape[0]))\n",
    "    # Feature: Normalized comment length (number of words)\n",
    "    n_words = np.zeros((dataset.shape[0]))\n",
    "    # Feature: Avg number of letters per words\n",
    "    letters_per_word = np.zeros((dataset.shape[0]))\n",
    "    # Feature: Number of punctuation signs per word (, . ! ? : ;)\n",
    "    punctuation_count = np.zeros((dataset.shape[0]))\n",
    "    punct = [',', '.', '!', '?', ':', ';']\n",
    "    # Feature: Most common word count\n",
    "    l = np.concatenate(dataset[\"text\"])\n",
    "    most_common_words = [word for word, word_count in Counter(l).most_common(nb_words)]\n",
    "    zeros = np.zeros(shape = (dataset.shape[0], nb_words))\n",
    "    word_count_features = pd.DataFrame(zeros, columns = most_common_words)    \n",
    "    # Feature: Misspelled words\n",
    "    misspelled_feature = np.zeros(dataset.shape[0])\n",
    "    spell = SpellChecker()\n",
    "    #Feature: Swear words\n",
    "    swear_words = pd.read_csv(\"swearWords.csv\")\n",
    "    s_words = np.zeros(dataset.shape[0])\n",
    "    \n",
    "    \n",
    "    # Iterate over comments\n",
    "    for i in range(dataset.shape[0]):\n",
    "        txt = dataset.iloc[i][\"text\"]\n",
    "        # Iterate over words\n",
    "        for w in txt:\n",
    "            # most common words\n",
    "            for target in most_common_words:\n",
    "                if w == target:\n",
    "                    word_count_features.iloc[i][target] += 1\n",
    "            \n",
    "            #swear words count\n",
    "            for target in swear_words:\n",
    "                if w == target:\n",
    "                    s_words[i]+=1\n",
    "            \n",
    "            # punctuation count\n",
    "            for x in punct:\n",
    "                punctuation_count[i] += w.count(x)\n",
    "                \n",
    "            # question counter\n",
    "            if \"?\" in w:\n",
    "                qmarks[i] = 1\n",
    "                \n",
    "            # comment length\n",
    "            n_words[i] += 1\n",
    "            # number of letters\n",
    "            letters_per_word[i] += len(w)\n",
    "            \n",
    "    # misspelled count   \n",
    "    for i in range(dataset.shape[0]):\n",
    "        new = [re.sub(r\"^\\W+|\\W+$\",\"\", word) for word in dataset.iloc[i][\"text\"]]\n",
    "        misspelled_words = spell.unknown(new)\n",
    "        misspelled_feature[i] = len(misspelled_words)\n",
    "    \n",
    "    # Get average number of letters per word\n",
    "    for i in range(dataset.shape[0]):\n",
    "        letters_per_word[i] = letters_per_word[i]/n_words[i]\n",
    "        \n",
    "    # Get average punctuation marks per word\n",
    "    for i in range(dataset.shape[0]):\n",
    "        punctuation_count[i] = punctuation_count[i]/n_words[i]\n",
    "        \n",
    "    # Add feature columns \n",
    "    # Most common words\n",
    "    dataset = pd.concat([dataset, word_count_features], axis=1)\n",
    "    # Misspelled words count\n",
    "    dataset = dataset.assign(misspelled=pd.Series(misspelled_feature).values)\n",
    "    # Swear words\n",
    "    dataset = dataset.assign(s_words=pd.Series(s_words).values)\n",
    "    # Question marks\n",
    "    dataset = dataset.assign(has_question=pd.Series(qmarks).values.astype(int))\n",
    "    # Avg letters per word\n",
    "    dataset = dataset.assign(letters_per_word=pd.Series(letters_per_word).values)\n",
    "    # Punctuation per word\n",
    "    dataset = dataset.assign(punctuation_count=pd.Series(punctuation_count).values)\n",
    "    #Add bias term\n",
    "    ones = np.ones((dataset.shape[0]))\n",
    "    dataset = dataset.assign(bias = pd.Series(ones).values)\n",
    "     #Drop text column\n",
    "    dataset = dataset.drop([\"text\"], axis=1)\n",
    "    #Move y value to the end\n",
    "    dataset = dataset[[\"bias\"] + [c for c in dataset if c not in [\"popularity_score\",\"bias\"]] + [\"popularity_score\"]]\n",
    "    return (dataset, most_common_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(dataset, words):\n",
    "    \n",
    "    # Feature: Does the comment contain a question mark\n",
    "    qmarks = np.zeros((dataset.shape[0]))\n",
    "    # Feature: Normalized comment length (number of words)\n",
    "    n_words = np.zeros((dataset.shape[0]))\n",
    "    # Feature: Avg number of letters per words\n",
    "    letters_per_word = np.zeros((dataset.shape[0]))\n",
    "    # Feature: Number of punctuation signs per word (, . ! ? : ;)\n",
    "    punctuation_count = np.zeros((dataset.shape[0]))\n",
    "    punct = [',', '.', '!', '?', ':', ';']\n",
    "    # Feature: Most common word count\n",
    "    most_common_words = words\n",
    "    zeros = np.zeros(shape = (dataset.shape[0], len(words)))\n",
    "    word_count_features = pd.DataFrame(zeros, columns = most_common_words)    \n",
    "    # Feature: Misspelled words\n",
    "    misspelled_feature = np.zeros(dataset.shape[0])\n",
    "    spell = SpellChecker()\n",
    "    #Feature: Swear words\n",
    "    swear_words = pd.read_csv(\"swearWords.csv\")\n",
    "    s_words = np.zeros(dataset.shape[0])\n",
    "    \n",
    "    \n",
    "    # Iterate over comments\n",
    "    for i in range(dataset.shape[0]):\n",
    "        txt = dataset.iloc[i][\"text\"]\n",
    "        # Iterate over words\n",
    "        for w in txt:\n",
    "            # most common words\n",
    "            for target in most_common_words:\n",
    "                if w == target:\n",
    "                    word_count_features.iloc[i][target] += 1\n",
    "            \n",
    "            #swear words count\n",
    "            for target in swear_words:\n",
    "                if w == target:\n",
    "                    s_words[i]+=1\n",
    "            \n",
    "            # punctuation count\n",
    "            for x in punct:\n",
    "                punctuation_count[i] += w.count(x)\n",
    "                \n",
    "            # question counter\n",
    "            if \"?\" in w:\n",
    "                qmarks[i] = 1\n",
    "                \n",
    "            # comment length\n",
    "            n_words[i] += 1\n",
    "            # number of letters\n",
    "            letters_per_word[i] += len(w)\n",
    "            \n",
    "    # misspelled count   \n",
    "    for i in range(dataset.shape[0]):\n",
    "        new = [re.sub(r'[^a-zA-Z]', '', x) for x in dataset.iloc[i][\"text\"]]\n",
    "        misspelled_words = spell.unknown(new)\n",
    "        misspelled_feature[i] = len(misspelled_words)\n",
    "    \n",
    "    # Get average number of letters per word\n",
    "    for i in range(dataset.shape[0]):\n",
    "        letters_per_word[i] = letters_per_word[i]/n_words[i]\n",
    "        \n",
    "    # Get average punctuation marks per word\n",
    "    for i in range(dataset.shape[0]):\n",
    "        punctuation_count[i] = punctuation_count[i]/n_words[i]\n",
    "                    \n",
    "    # Add feature columns \n",
    "    # Most common words\n",
    "    dataset = pd.concat([dataset, word_count_features], axis=1)\n",
    "    # Misspelled words count\n",
    "    dataset = dataset.assign(misspelled=pd.Series(misspelled_feature).values)\n",
    "    # Swear words\n",
    "    dataset = dataset.assign(s_words=pd.Series(s_words).values)\n",
    "    # Question marks\n",
    "    dataset = dataset.assign(has_question=pd.Series(qmarks).values.astype(int))\n",
    "    # Avg letters per word\n",
    "    dataset = dataset.assign(letters_per_word=pd.Series(letters_per_word).values)\n",
    "    # Punctuation per word\n",
    "    dataset = dataset.assign(punctuation_count=pd.Series(punctuation_count).values)\n",
    "    #Add bias term\n",
    "    ones = np.ones((dataset.shape[0]))\n",
    "    dataset= dataset.assign(bias = pd.Series(ones).values)\n",
    "    #Drop text column\n",
    "    dataset = dataset.drop([\"text\"], axis=1)\n",
    "    #Move y value to the end\n",
    "    dataset = dataset[[\"bias\"] +[c for c in dataset if c not in [\"popularity_score\"]] + [\"popularity_score\"]]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, words = preprocess(train,5)\n",
    "train = train.values.astype(float)\n",
    "validation = feature_extraction(validation, words).values.astype(float)\n",
    "test = feature_extraction(test, words).values.astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train[0,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Jenny/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "def sentiment(dataset, words):\n",
    "    l = reduce(operator.concat, dataset[\"text\"])\n",
    "    most_common_words = [word for word, word_count in Counter(l).most_common(words)]\n",
    "    stop = stopwords.words('english')\n",
    "    for i in range(dataset.shape[0]):\n",
    "        txt = dataset.iloc[i][\"text\"]\n",
    "        for w in txt:\n",
    "            if w in stop:\n",
    "                txt.remove(w)\n",
    "    #remove most common words\n",
    "            if w in most_common_words:\n",
    "                txt.remove(w)\n",
    "    #Correct misspelled\n",
    "           # w.correct()\n",
    "    #Lemmatization\n",
    "            #w.lemmatize()\n",
    "    #Sentiment analysis\n",
    "        polarity, subjectivity = txt.sentiment()\n",
    "        \n",
    "    train['sentiment'] = train['tweet'].apply(lambda x: TextBlob(x).sentiment[0] )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GD import gradient_descent\n",
    "def Linear_regression(X, y, method = 0, w_0 = None, alpha_0 = 1, b = 1, eps = 1e-06):\n",
    "\n",
    "    # local variables\n",
    "    p = X.shape[1]\n",
    "    optim_w = np.zeros(p)\n",
    "\n",
    "    # computes the optimal weights using the closed form solution\n",
    "    if(method == 0):\n",
    "        X_T = X.T\n",
    "        b = X_T @ y\n",
    "        A = X_T @ X\n",
    "        \n",
    "        optim_w = np.linalg.solve(A,b)\n",
    "\n",
    "        return optim_w\n",
    "\n",
    "    # computes the optimal weights using gradient descent\n",
    "    else:\n",
    "        if w_0 is None:\n",
    "            w_0 = np.zeros(p)\n",
    "\n",
    "        optim_w = gradient_descent(X, y, w_0, alpha_0, b, eps)\n",
    "\n",
    "        return optim_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_0, alpha_0, b, eps):\n",
    "\n",
    "    # local variables\n",
    "    i = 0 # iterations performed\n",
    "    alpha = alpha_0 # step size\n",
    "    prev_w = w_0 # weight of previous iteration\n",
    "    current_w = w_0 # weight of current iteration\n",
    "\n",
    "    # precomputes terms used in the gradient descent update\n",
    "    X_T = X.T\n",
    "    crossprod_X = X_T @ X\n",
    "    y_term = X_T @ y\n",
    "\n",
    "    # performs gradient descent until stopping condition reached\n",
    "    while True:\n",
    "\n",
    "        # updates the step size\n",
    "        alpha = alpha_0/(1 + b*i)\n",
    "\n",
    "        # updates the weights\n",
    "        prev_w = current_w\n",
    "        current_w = current_w - 2 * alpha * (crossprod_X @ current_w - y_term)\n",
    "\n",
    "        # updates the iteration number\n",
    "        i = i + 1\n",
    "\n",
    "        # checks the stopping condition\n",
    "        if np.linalg.norm(current_w - prev_w) < eps:\n",
    "            break\n",
    "\n",
    "    # returns the optimal weights\n",
    "    return current_w\n",
    "\n",
    "from sympy.solvers import solve\n",
    "def Linear_regression(X, y, method = 0, w_0 = None, alpha_0 = 1, b = 1, eps = 1e-06):\n",
    "\n",
    "    # local variables\n",
    "    p = X.shape[1]\n",
    "    optim_w = np.zeros(p)\n",
    "\n",
    "    # computes the optimal weights using the closed form solution\n",
    "    if(method == 0):\n",
    "        X_T = X.T\n",
    "        b = X_T @ y\n",
    "        A = X_T @ X\n",
    "        \n",
    "        optim_w = np.linalg.solve(A,b)\n",
    "\n",
    "        return optim_w\n",
    "\n",
    "    # computes the optimal weights using gradient descent\n",
    "    else:\n",
    "        if w_0 is None:\n",
    "            w_0 = np.zeros(p)\n",
    "\n",
    "        optim_w = gradient_descent(X, y, w_0, alpha_0, b, eps)\n",
    "\n",
    "        return optim_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8.45983041e-01  3.75533780e-01 -1.09554785e+00 -2.33466789e-01\n",
      " -3.34379529e-03 -4.06777600e-04 -7.57469048e-03  8.38157340e-03\n",
      "  2.98382428e-02 -3.79098679e-03  6.19035359e-02 -1.00117708e-01\n",
      " -5.17723460e-03  2.13392348e-02]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 8.45983041e-01,  3.75533780e-01, -1.09554785e+00, -2.33466789e-01,\n",
       "       -3.34379529e-03, -4.06777600e-04, -7.57469048e-03,  8.38157340e-03,\n",
       "        2.98382428e-02, -3.79098679e-03,  6.19035359e-02, -1.00117708e-01,\n",
       "       -5.17723460e-03,  2.13392348e-02])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train[:,:-1]\n",
    "y = train[:,-1]\n",
    "linreg = Linear_regression(X, y, method = 0)\n",
    "print(linreg)\n",
    "Linear_regression(X, y, method = 1, alpha_0 = 1e-06, b = 0, eps = 1e-15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-bcb18963435c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-b57b01113c79>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(dataset, nb_words)\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mpunct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'!'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m':'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m';'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Feature: Most common word count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mmost_common_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_count\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mzeros\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "preprocess(train, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 14)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
