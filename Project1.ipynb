{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "import operator\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "\n",
    "#Load data in json format and store in a dataframe\n",
    "with open(\"proj1_data.json\") as fp:\n",
    "    data = json.load(fp)\n",
    "    df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert true-false to 1-0\n",
    "df[\"is_root\"] = df[\"is_root\"].astype(int)\n",
    "#Convert all text to lower cases\n",
    "df[\"text\"]= [x.lower() for x in df[\"text\"]]\n",
    "#Parse text where there's a space\n",
    "df[\"text\"]= [x.split() for x in df[\"text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>children</th>\n",
       "      <th>controversiality</th>\n",
       "      <th>is_root</th>\n",
       "      <th>popularity_score</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.254698</td>\n",
       "      <td>[its, raining, sideways]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.509813</td>\n",
       "      <td>[wheel, of, time, reader, confirmed!]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.370827</td>\n",
       "      <td>[the, jungle, book, of, pussy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.272843</td>\n",
       "      <td>[i'm, just, making, this, thread, since, there...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.560150</td>\n",
       "      <td>[hi, there,, looks, like, you're, wanting, to,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.696554</td>\n",
       "      <td>[when, there, is, a, line, to, exit, a, store,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.050417</td>\n",
       "      <td>[it, is, none, of, their, business.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.310543</td>\n",
       "      <td>[infinite, ammo, and, the, absurd, number, of,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.208735</td>\n",
       "      <td>[if, you're, trying, to, argue, for, a, claim,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.123700</td>\n",
       "      <td>[oh, god, it's, all, my, cousins, from, the, s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   children  controversiality  is_root  popularity_score  \\\n",
       "0         0                 0        0          1.254698   \n",
       "1         0                 0        0          0.509813   \n",
       "2         0                 0        1          0.370827   \n",
       "3         0                 0        0         -0.272843   \n",
       "4         0                 0        1          0.560150   \n",
       "5         0                 0        1          0.696554   \n",
       "6         0                 0        1          1.050417   \n",
       "7         1                 0        1          0.310543   \n",
       "8         1                 1        0         -1.208735   \n",
       "9         1                 0        0          1.123700   \n",
       "\n",
       "                                                text  \n",
       "0                           [its, raining, sideways]  \n",
       "1              [wheel, of, time, reader, confirmed!]  \n",
       "2                     [the, jungle, book, of, pussy]  \n",
       "3  [i'm, just, making, this, thread, since, there...  \n",
       "4  [hi, there,, looks, like, you're, wanting, to,...  \n",
       "5  [when, there, is, a, line, to, exit, a, store,...  \n",
       "6               [it, is, none, of, their, business.]  \n",
       "7  [infinite, ammo, and, the, absurd, number, of,...  \n",
       "8  [if, you're, trying, to, argue, for, a, claim,...  \n",
       "9  [oh, god, it's, all, my, cousins, from, the, s...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First 10000 data points as training set\n",
    "train = df.iloc[0:10000,:]\n",
    "#10000 to 11000 as validation set\n",
    "validation = df.iloc[10000:11000,:]\n",
    "#Last 1000 as test set\n",
    "test = df.iloc[11000:12000,:]\n",
    "#Display the first 10 points of the training data\n",
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(dataset,words):\n",
    "    \n",
    "    # Feature: Does the comment contain a question mark\n",
    "    qmarks = np.zeros((dataset.shape[0]))\n",
    "    # Feature: Normalized comment length (number of words)\n",
    "    n_words = np.zeros((dataset.shape[0]))\n",
    "    # Feature: Avg number of letters per words\n",
    "    letters_per_word = np.zeros((dataset.shape[0]))\n",
    "    # Feature: Number of punctuation signs per word (, . ! ? : ;)\n",
    "    punctuation_count = np.zeros((dataset.shape[0]))\n",
    "    punct = [',', '.', '!', '?', ':', ';']\n",
    "    # Feature: Most common word count\n",
    "    l = reduce(operator.concat, dataset[\"text\"])\n",
    "    most_common_words = [word for word, word_count in Counter(l).most_common(words)]\n",
    "    zeros = np.zeros(shape = (dataset.shape[0], words))\n",
    "    word_count_features = pd.DataFrame(zeros, columns = most_common_words)    \n",
    "    # Feature: Misspelled words\n",
    "    misspelled_feature = np.zeros(dataset.shape[0])\n",
    "    spell = SpellChecker()\n",
    "    #Feature: Swear words\n",
    "    swear_words = pd.read_csv(\"swearWords.csv\")\n",
    "    s_words = np.zeros(dataset.shape[0])\n",
    "    \n",
    "    \n",
    "    # Iterate over comments\n",
    "    for i in range(dataset.shape[0]):\n",
    "        txt = dataset.iloc[i][\"text\"]\n",
    "        # Iterate over words\n",
    "        for w in txt:\n",
    "            # most common words\n",
    "            for target in most_common_words:\n",
    "                if w == target:\n",
    "                    word_count_features.iloc[i][target] += 1\n",
    "            \n",
    "            #swear words count\n",
    "            for target in swear_words:\n",
    "                if w == target:\n",
    "                    s_words[i]+=1\n",
    "            \n",
    "            # punctuation count\n",
    "            for x in punct:\n",
    "                punctuation_count[i] += w.count(x)\n",
    "                \n",
    "            # question counter\n",
    "            if \"?\" in w:\n",
    "                qmarks[i] = 1\n",
    "                \n",
    "            # comment length\n",
    "            n_words[i] += 1\n",
    "            # number of letters\n",
    "            letters_per_word[i] += len(w)\n",
    "            \n",
    "    # misspelled count   \n",
    "    for i in range(dataset.shape[0]):\n",
    "        new = [re.sub(r'[^a-zA-Z]', '', x) for x in dataset.iloc[i][\"text\"]]\n",
    "        misspelled_words = spell.unknown(new)\n",
    "        misspelled_feature[i] = len(misspelled_words)\n",
    "    \n",
    "    # Get average number of letters per word\n",
    "    for i in range(dataset.shape[0]):\n",
    "        letters_per_word[i] = letters_per_word[i]/n_words[i]\n",
    "        \n",
    "    # Get average punctuation marks per word\n",
    "    for i in range(dataset.shape[0]):\n",
    "        punctuation_count[i] = punctuation_count[i]/n_words[i]\n",
    "    \n",
    "    # Normalize word length (divide by average)\n",
    "    n_words = n_words/np.mean(n_words)\n",
    "                \n",
    "    # Add feature columns \n",
    "    # Most common words\n",
    "    dataset = pd.concat([dataset, word_count_features], axis=1)\n",
    "    # Misspelled words count\n",
    "    dataset = dataset.assign(misspelled=pd.Series(misspelled_feature).values)\n",
    "    # Swear words\n",
    "    dataset = dataset.assign(s_words=pd.Series(s_words).values)\n",
    "    # Question marks\n",
    "    dataset = dataset.assign(has_question=pd.Series(qmarks).values.astype(int))\n",
    "    # Number of words\n",
    "    dataset = dataset.assign(n_words=pd.Series(n_words).values)\n",
    "    # Avg letters per word\n",
    "    dataset = dataset.assign(letters_per_word=pd.Series(letters_per_word).values)\n",
    "    # Punctuation per word\n",
    "    dataset = dataset.assign(punctuation_count=pd.Series(punctuation_count).values)\n",
    "                \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = preprocess(train,5)\n",
    "validation = preprocess(validation, 5)\n",
    "test = preprocess(test,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/Jenny/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "def sentiment(dataset, words):\n",
    "    l = reduce(operator.concat, dataset[\"text\"])\n",
    "    most_common_words = [word for word, word_count in Counter(l).most_common(words)]\n",
    "    stop = stopwords.words('english')\n",
    "    for i in range(dataset.shape[0]):\n",
    "        txt = dataset.iloc[i][\"text\"]\n",
    "        for w in txt:\n",
    "            if w in stop:\n",
    "                txt.remove(w)\n",
    "    #remove most common words\n",
    "            if w in most_common_words:\n",
    "                txt.remove(w)\n",
    "    #Correct misspelled\n",
    "           # w.correct()\n",
    "    #Lemmatization\n",
    "            #w.lemmatize()\n",
    "    #Sentiment analysis\n",
    "        polarity, subjectivity = txt.sentiment()\n",
    "        \n",
    "    train['sentiment'] = train['tweet'].apply(lambda x: TextBlob(x).sentiment[0] )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sympy.solvers import solve\n",
    "def Linear_regression(X, y, method = 0, w_0 = None, alpha_0 = 1, b = 1, eps = 1e-06):\n",
    "\n",
    "    # local variables\n",
    "    p = X.shape[1]\n",
    "    optim_w = np.zeros(p)\n",
    "\n",
    "    # computes the optimal weights using the closed form solution\n",
    "    if(method == 0):\n",
    "        X_T = X.T\n",
    "        b = X_T @ y\n",
    "        A = X_T @ X\n",
    "        \n",
    "        optim_w = np.linalg.inv(A).dot(b)\n",
    "\n",
    "        return optim_w\n",
    "\n",
    "    # computes the optimal weights using gradient descent\n",
    "    else:\n",
    "        if w_0 is None:\n",
    "            w_0 = np.zeros(p)\n",
    "\n",
    "        optim_w = gradient_descent(X, y, w_0, alpha_0, b, eps)\n",
    "\n",
    "        return optim_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_0, alpha_0, b, eps):\n",
    "\n",
    "    # local variables\n",
    "    i = 0 # iterations performed\n",
    "    alpha = n_0 # step size\n",
    "    prev_w = w_0 # weight of previous iteration\n",
    "    current_w = w_0 # weight of current iteration\n",
    "\n",
    "    # precomputes terms used in the gradient descent update\n",
    "    X_T = X.T\n",
    "    crossprod_X = X_T @ X\n",
    "    y_term = X_T @ y\n",
    "\n",
    "    # performs gradient descent until stopping condition reached\n",
    "    while True:\n",
    "\n",
    "        # updates the step size\n",
    "        alpha = n_0/(1 + b*i)\n",
    "\n",
    "        # updates the weights\n",
    "        prev_w = current_w\n",
    "        current_w = current_w - 2 * alpha * (crossprod_X @ current_w - y_term)\n",
    "\n",
    "        # updates the iteration number\n",
    "        i = i + 1\n",
    "\n",
    "        # checks the stopping condition\n",
    "        if np.norm(current_w - prev_w) < eps:\n",
    "            break\n",
    "\n",
    "    # returns the optimal weights\n",
    "    return current_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.33768243e+01,  5.27350628e+02, -7.15620197e-01,  4.06140834e+01,\n",
       "       -9.37555583e+01,  1.15952807e+01,  3.69383762e+00,  7.65041510e+01,\n",
       "        7.05733811e+01,  8.40381180e+01, -1.87684890e+02,  4.47554672e+01,\n",
       "        1.55978201e+02,  2.58963108e-02,  6.56293811e-02,  1.24272171e-01,\n",
       "        3.18137307e-01,  5.82797811e-02,  3.90278112e-02, -5.08026246e+01,\n",
       "        8.85137063e+01, -7.64224492e+00, -3.51753457e+00, -6.85903178e+01,\n",
       "       -8.00458461e+01, -9.08274214e+01,  1.91486547e+02, -4.80595959e+01,\n",
       "       -1.52640296e+02,  1.18021895e-02, -2.91371672e-02,  3.01110970e-02,\n",
       "       -1.13103770e-02, -5.49825020e-02, -8.70566682e-02,  4.88318424e-02,\n",
       "        1.83402413e-02,  1.74819006e-02,  1.04053895e-04,  1.65199119e-02,\n",
       "       -3.94150722e-02, -9.51721196e-02, -6.83505481e-02,  3.68610550e-02,\n",
       "        7.02168227e-02, -3.85142764e-02,  8.05563146e-03, -9.18425274e-03,\n",
       "        1.32054775e-02, -2.15889063e-03, -5.22676288e-02,  7.83873578e-03,\n",
       "        3.15621728e-02,  4.70074662e-02, -3.88730687e-02, -4.75906085e-02,\n",
       "        1.53715006e-02, -3.17327910e-02, -3.87016157e-02, -7.83455169e-02,\n",
       "       -1.30113877e-03,  1.41456256e-02, -6.40756609e-02, -7.15408716e-02,\n",
       "        8.12297155e-02, -2.63952133e-02,  1.93321409e-03,  4.64017918e-02,\n",
       "        5.82950499e-02, -2.85144796e-02, -3.97819703e-02, -5.17895691e-02,\n",
       "       -4.27385702e-02, -9.77730201e-03, -2.34323071e-01, -3.16129613e-02,\n",
       "       -9.21499834e-02, -4.89108543e-02, -9.99100323e-02,  1.43951696e-01,\n",
       "       -9.11683453e-02,  5.46940815e-02, -6.98141586e-02,  6.46551150e-02,\n",
       "       -9.76626873e-03, -3.51330725e-02, -4.66174522e-02, -5.34506357e-02,\n",
       "       -7.91375035e-02, -8.60408199e-02, -1.21555598e-02, -9.75497401e-02,\n",
       "       -5.05337138e-03, -5.61552423e-03, -1.26051916e-01,  6.18862883e-03,\n",
       "        1.59057532e-02, -2.67302607e-02, -5.43377734e-02, -6.35065246e-02,\n",
       "        1.10164962e-01,  1.13562880e-01,  7.08376642e-03, -1.95075726e-02,\n",
       "        9.33307423e-02,  8.01210055e-03,  5.75691764e-02, -1.07687216e-01,\n",
       "        2.87626361e-02, -1.28012356e-01,  8.15526928e-02,  5.97487419e-03,\n",
       "        1.59802059e-02, -6.07438446e-02, -4.14337779e-02,  1.03923297e-01,\n",
       "       -4.31556585e-02, -3.25763402e-02, -2.20068420e-02,  1.63358528e-01,\n",
       "        9.46245663e-03, -1.60916287e-01,  7.13180978e-02,  5.93631897e-02,\n",
       "        8.18780267e-03, -7.85554277e-04, -1.06355752e-02, -1.43194369e-01,\n",
       "       -2.03239590e-02,  4.90854400e-02,  2.60396230e-02,  1.36539044e-01,\n",
       "        3.56418036e-03, -6.93837791e-02, -8.85036338e-02, -6.69897422e-02,\n",
       "        9.09782187e-02,  7.54427695e-02,  6.98208982e-02,  5.92783117e-02,\n",
       "       -2.40607839e-02,  1.21196747e-01, -6.92187746e-02,  7.84119584e-02,\n",
       "        1.39337091e-03,  1.54548732e-02, -2.61129749e-01, -9.06916669e-02,\n",
       "        3.23549899e-02,  1.42401685e-01, -3.60640642e-01,  2.45263574e-02,\n",
       "       -7.38140514e-03,  1.09317610e-01, -6.58397932e-03,  3.35036376e-02,\n",
       "        5.65552702e-04,  4.26144481e-02,  1.61338323e-01, -5.82792881e-02,\n",
       "        1.85705967e-02, -3.28375442e-02,  8.89999361e-02,  7.18749727e-02,\n",
       "       -1.05103864e-01, -9.96588374e-02,  6.16960073e-02,  2.97803167e-01,\n",
       "       -5.55244534e-01, -1.80882075e-01, -4.40854317e-02, -3.05688917e-02,\n",
       "        2.92120689e-02,  1.01975550e-01, -3.88024032e-01, -9.52792187e-03,\n",
       "        6.06406659e-02,  3.22806560e-02,  4.19541376e-02])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = train.drop([\"popularity_score\",\"text\"], axis=1)\n",
    "y = train[\"popularity_score\"]\n",
    "linreg = Linear_regression(X, y, method = 0, w_0 = None, alpha_0 = 0.1, b = 1, eps = 1e-06)\n",
    "linreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(train,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
